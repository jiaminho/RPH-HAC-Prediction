{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f63e01a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jiaminho/RPH-HAC-Prediction/blob/jiamin_branch/HAC_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nrYbP44CHIV0",
   "metadata": {
    "id": "nrYbP44CHIV0"
   },
   "source": [
    "# Hospital Acquired Complications (HACs) Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6IHVg8tHcLu",
   "metadata": {
    "id": "E6IHVg8tHcLu"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hFxLDoGqHH3t",
   "metadata": {
    "id": "hFxLDoGqHH3t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "pd.set_option('display.max_columns', None)  # None ensures all columns are shown\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cY4ZYAmfJPqb",
   "metadata": {
    "id": "cY4ZYAmfJPqb"
   },
   "source": [
    "Read pre-processed data \"all_encoded_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Je11z70nHi-U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Je11z70nHi-U",
    "outputId": "03d4f2f8-c016-46d6-ee92-28b6619738c2"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate and create the PyDrive client\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Download the file\n",
    "file_id = '1-9Tp6ZMZG9zKieBTO_Sd6W-HdzXPy5OL'\n",
    "file_name = 'selected_features_500.csv'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile(file_name)\n",
    "\n",
    "# Read into DataFrame\n",
    "selected_features_df = pd.read_csv(file_name)\n",
    "selected_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc09aa1a-581a-49b2-a927-d073c031a447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features_df = pd.read_csv('selected_features_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5747466f-1464-4c33-b61f-1288c73e2841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features_df['uti'] = selected_features_df['uti'].replace({True: 1, False: 0})\n",
    "selected_features_df = selected_features_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9u2CfKd5F7QO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "9u2CfKd5F7QO",
    "outputId": "d50b50ea-ee7a-49c6-8b40-0777f077ae70",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_df.head()\n",
    "selected_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixp0_f-RLQ-3",
   "metadata": {
    "id": "ixp0_f-RLQ-3"
   },
   "source": [
    "train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "qGUCL6mFJNkn",
   "metadata": {
    "id": "qGUCL6mFJNkn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = selected_features_df.drop('uti', axis=1)\n",
    "y = selected_features_df['uti']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9vRcI9mOMA0G",
   "metadata": {
    "id": "9vRcI9mOMA0G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "# scaler_original = StandardScaler()\n",
    "# X_train_scaled = scaler_original.fit_transform(X_train)\n",
    "# X_test_scaled = scaler_original.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qXy_TIphATZk",
   "metadata": {
    "id": "qXy_TIphATZk"
   },
   "source": [
    "train val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a407fe-775c-4874-b5e1-ea542f7b22a6",
   "metadata": {},
   "source": [
    "### DNN (Leon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385faf87-3daf-4c99-8b5e-5992d3950289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "sample_weights\n",
    "\n",
    "# Performance Scheduling\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# early stopping\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # val_loss is the defaul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bf4b661-f8dc-498d-b582-e21995931a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Define the F1 score metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "# Define the balanced accuracy metric\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_pred_labels = K.round(y_pred)\n",
    "    recall_pos = K.sum(K.round(K.clip(y_true * y_pred_labels, 0, 1))) / (K.sum(K.round(K.clip(y_true, 0, 1))) + K.epsilon())\n",
    "    recall_neg = K.sum(K.round(K.clip((1-y_true) * (1-y_pred_labels), 0, 1))) / (K.sum(K.round(K.clip(1-y_true, 0, 1))) + K.epsilon())\n",
    "    balanced_acc = (recall_pos + recall_neg) / 2.0\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67cdd7-d316-43a1-b26c-805711d65ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=2, max_value=3, step=1)  # 2 possible options\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=28, max_value=31, step=1)  # 4 possible options\n",
    "    \n",
    "    learning_rate = 0.001  # constant lr\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    kernel_initializer = hp.Choice(\"kernel_initializer\", values=[\"he_normal\", \"glorot_uniform\"])  # 2 possible outcomes\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "    \n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation='swish', kernel_initializer=kernel_initializer))\n",
    "        tf.keras.layers.Dropout(0.2),  # Dropout layer added here\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c5e08c0-f527-494e-813d-50a1fdccd6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not infer optimization direction (\"min\" or \"max\") for unknown metric \"val_balanced_accuracy\". Please specify the objective  asa `keras_tuner.Objective`, for example `keras_tuner.Objective(\"val_balanced_accuracy\", direction=\"min\")`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     21\u001b[0m                   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[balanced_accuracy])\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 26\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mBayesianOptimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_balanced_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutions_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeras_tuner_bayesian_demo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m tmp_model \u001b[38;5;241m=\u001b[39m build_model(tuner\u001b[38;5;241m.\u001b[39mhyperparameters)\n\u001b[1;32m     36\u001b[0m norm_layer \u001b[38;5;241m=\u001b[39m tmp_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/tuners/bayesian.py:381\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    367\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    380\u001b[0m ):\n\u001b[0;32m--> 381\u001b[0m     oracle \u001b[38;5;241m=\u001b[39m \u001b[43mBayesianOptimizationOracle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune_new_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune_new_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_new_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_new_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_consecutive_failed_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_consecutive_failed_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(oracle\u001b[38;5;241m=\u001b[39moracle, hypermodel\u001b[38;5;241m=\u001b[39mhypermodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/tuners/bayesian.py:115\u001b[0m, in \u001b[0;36mBayesianOptimizationOracle.__init__\u001b[0;34m(self, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, allow_new_entries, tune_new_entries, max_retries_per_trial, max_consecutive_failed_trials)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sklearn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install scikit-learn (sklearn) before using the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`BayesianOptimization` with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install keras-tuner[bayesian]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune_new_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune_new_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_new_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_new_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_consecutive_failed_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_consecutive_failed_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_initial_points \u001b[38;5;241m=\u001b[39m num_initial_points\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m alpha\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py:179\u001b[0m, in \u001b[0;36mOracle.__init__\u001b[0;34m(self, objective, max_trials, hyperparameters, allow_new_entries, tune_new_entries, seed, max_retries_per_trial, max_consecutive_failed_trials)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    170\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     max_consecutive_failed_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    178\u001b[0m ):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m \u001b[43mobj_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_trials \u001b[38;5;241m=\u001b[39m max_trials\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hyperparameters:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/objective.py:153\u001b[0m, in \u001b[0;36mcreate_objective\u001b[0;34m(objective)\u001b[0m\n\u001b[1;32m    146\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not infer optimization direction (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor unknown metric \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{obj}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please specify the objective  as\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma `keras_tuner.Objective`, for example `keras_tuner.Objective(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{obj}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, direction=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m error_msg\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m=\u001b[39mobjective)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Objective(name\u001b[38;5;241m=\u001b[39mobjective, direction\u001b[38;5;241m=\u001b[39mdirection)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not infer optimization direction (\"min\" or \"max\") for unknown metric \"val_balanced_accuracy\". Please specify the objective  asa `keras_tuner.Objective`, for example `keras_tuner.Objective(\"val_balanced_accuracy\", direction=\"min\")`."
     ]
    }
   ],
   "source": [
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "        norm_layer,\n",
    "        tf.keras.layers.Dense(hp.Int('dense_units', min_value=2, max_value=10, step=1), \n",
    "                              activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout_rate', 0.1, 0.5, step=0.1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(hp.Int('dense_units_2', min_value=2, max_value=10, step=1),\n",
    "                              activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout_rate_2', 0.1, 0.5, step=0.1)),\n",
    "        tf.keras.layers.Dense(hp.Int('dense_units_3', min_value=2, max_value=10, step=1),\n",
    "                              activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout_rate_3', 0.1, 0.5, step=0.1)),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[balanced_accuracy])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=('val_balanced_accuracy', direction=\"max\"),\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='output_dir',\n",
    "    project_name='keras_tuner_bayesian_demo'\n",
    ")\n",
    "\n",
    "tmp_model = build_model(tuner.hyperparameters)\n",
    "norm_layer = tmp_model.layers[0]\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=20,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd55e9-f491-4d0d-bb7e-4ef591233fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c4acb1e-1707-4a55-955b-6808dc1be5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 2s 9ms/step - loss: 0.6879 - balanced_accuracy: 0.5454 - val_loss: 0.6645 - val_balanced_accuracy: 0.6416 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6663 - balanced_accuracy: 0.5885 - val_loss: 0.6456 - val_balanced_accuracy: 0.7013 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6471 - balanced_accuracy: 0.6274 - val_loss: 0.6178 - val_balanced_accuracy: 0.7390 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6237 - balanced_accuracy: 0.6579 - val_loss: 0.5820 - val_balanced_accuracy: 0.7658 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6054 - balanced_accuracy: 0.6682 - val_loss: 0.5563 - val_balanced_accuracy: 0.7754 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5915 - balanced_accuracy: 0.6814 - val_loss: 0.5330 - val_balanced_accuracy: 0.7875 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5676 - balanced_accuracy: 0.7142 - val_loss: 0.5085 - val_balanced_accuracy: 0.7999 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.5508 - balanced_accuracy: 0.7353 - val_loss: 0.4896 - val_balanced_accuracy: 0.8012 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.5358 - balanced_accuracy: 0.7526 - val_loss: 0.4735 - val_balanced_accuracy: 0.8065 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5140 - balanced_accuracy: 0.7737 - val_loss: 0.4605 - val_balanced_accuracy: 0.8127 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4990 - balanced_accuracy: 0.7787 - val_loss: 0.4493 - val_balanced_accuracy: 0.8053 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4912 - balanced_accuracy: 0.7771 - val_loss: 0.4403 - val_balanced_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4697 - balanced_accuracy: 0.7974 - val_loss: 0.4369 - val_balanced_accuracy: 0.8165 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4686 - balanced_accuracy: 0.7985 - val_loss: 0.4362 - val_balanced_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4625 - balanced_accuracy: 0.8032 - val_loss: 0.4396 - val_balanced_accuracy: 0.8095 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.4479 - balanced_accuracy: 0.8118 - val_loss: 0.4331 - val_balanced_accuracy: 0.8114 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4591 - balanced_accuracy: 0.8015 - val_loss: 0.4330 - val_balanced_accuracy: 0.8145 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.4466 - balanced_accuracy: 0.8128 - val_loss: 0.4300 - val_balanced_accuracy: 0.8145 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4335 - balanced_accuracy: 0.8156 - val_loss: 0.4313 - val_balanced_accuracy: 0.8124 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4315 - balanced_accuracy: 0.8190 - val_loss: 0.4291 - val_balanced_accuracy: 0.8171 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(3, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout layer added here\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(3, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout layer added here\n",
    "    tf.keras.layers.Dense(3, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout layer added here\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_balanced_accuracy', mode='max',\n",
    "                                                     patience=10, restore_best_weights=True) \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[balanced_accuracy])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=64, sample_weight=sample_weights,\n",
    "                    callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75da1133-86b4-4979-9b89-4cc4a7ef173e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n",
      "200/200 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       870\n",
      "           1       0.86      0.79      0.83      1130\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and test predictions\n",
    "dnn_y_test_pred = model.predict(X_test)\n",
    "dnn_y_train_pred = model.predict(X_train)\n",
    "print(classification_report(y_test, dnn_y_test_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2758756e-d0eb-4db0-a25d-55ab27ad4606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.23% - Balanced accuracy Test\n",
      "88.52% - Balanced accuracy Train\n"
     ]
    }
   ],
   "source": [
    "# calculate balanced accuracy\n",
    "print(f\"{balanced_accuracy_score(y_test, dnn_y_test_pred.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, dnn_y_train_pred.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5PwmzrDkWLSL",
   "metadata": {
    "id": "5PwmzrDkWLSL"
   },
   "source": [
    "### DNN (Shiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Op44Jl_UhmgO",
   "metadata": {
    "id": "Op44Jl_UhmgO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims = 500\n",
    "\n",
    "X_train_scaled_new = X_train_scaled[:,:dims]\n",
    "X_val_new = X_val[:,:dims]\n",
    "X_test_scaled_new = X_test_scaled[:,:dims]\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Qjntb7AzyYk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Qjntb7AzyYk",
    "outputId": "7fec4d43-c1ef-4173-9881-2ca3467d3a53"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=[dims,])\n",
    "out = inp\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Performance Scheduling\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "for ix in range(3):\n",
    "  out = Dense(64,activation='swish')(out)\n",
    "  out = Dropout(0.2)(out)\n",
    "out_encoder = Dense(64)(out)\n",
    "out = Dense(1,activation='sigmoid')(out)\n",
    "model = Model(inp,out)\n",
    "model_encoder = Model(inp,out_encoder)\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train_scaled_new, y_train, epochs=100, batch_size=64, validation_data=(X_val_new, y_val),sample_weight=sample_weights,\n",
    "                   callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EIxbBYFKIZ4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIxbBYFKIZ4b",
    "outputId": "48513ecb-e9c2-4589-d662-90f5f481d700"
   },
   "outputs": [],
   "source": [
    "# train and test predictions\n",
    "dnn_y_test_pred = model.predict(X_test_scaled_new)\n",
    "dnn_y_train_pred = model.predict(X_train_scaled_new)\n",
    "print(classification_report(y_test, dnn_y_test_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CNoYubIwIbxq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNoYubIwIbxq",
    "outputId": "94f7f7b8-57ce-4246-8103-e0a746ff90f8"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "print(f\"{balanced_accuracy_score(y_test, dnn_y_test_pred.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, dnn_y_train_pred.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L4i2vQQ4POTJ",
   "metadata": {
    "id": "L4i2vQQ4POTJ"
   },
   "source": [
    "#### UMAP and Agglomerative clustering\n",
    "\n",
    "- Use a pre-trained model encoder to transform the training and testing data (X_train_scaled_new and X_test_scaled_new).\n",
    "- Apply UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction to the encoded test data.\n",
    "- Use agglomerative clustering on the encoded test data.\n",
    "- Identify clusters that contain prediction errors and summarizes these errors.\n",
    "- Visualize the data points in the 2D UMAP space, highlighting errors and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WSzNjl8aPLX6",
   "metadata": {
    "id": "WSzNjl8aPLX6"
   },
   "outputs": [],
   "source": [
    "# Installing Packages\n",
    "!pip install umap-learn\n",
    "!pip install bokeh\n",
    "!pip install bokeh holoviews colorcet scikit-image datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nMEnLNPWIbz2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMEnLNPWIbz2",
    "outputId": "14fa684f-163e-4f3d-f886-aaaa2942eae6"
   },
   "outputs": [],
   "source": [
    "# Using a Pre-trained Model Encoder to Transform Data\n",
    "Xenc_train = model_encoder(X_train_scaled_new).numpy()\n",
    "Xenc_test = model_encoder(X_test_scaled_new)\n",
    "\n",
    "# Applying UMAP Dimensionality Reduction\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(Xenc_test)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = model(X_test_scaled_new).numpy().round()\n",
    "\n",
    "# Fitting UMAP to Test Data\n",
    "mapper = umap.UMAP().fit(Xenc_test)\n",
    "\n",
    "# Applying Agglomerative Clustering\n",
    "cluster = AgglomerativeClustering(metric=\"cosine\", distance_threshold=0.5, n_clusters=None, linkage='average')\n",
    "cluster_test = cluster.fit_predict(Xenc_test.numpy())\n",
    "\n",
    "# Identifying Errors\n",
    "error = [cluster_test[ix] for ix in range(len(y_pred[:,0])) if (not y_pred[ix,0] == y_test.values[ix]) and  y_pred[ix,0] == 1]\n",
    "print(Counter(cluster_test).most_common(10))\n",
    "print(Counter(error).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pQTm8kLDPoj8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pQTm8kLDPoj8",
    "outputId": "699a9e7a-c680-47a9-fc8e-05c0cd00ef5a"
   },
   "outputs": [],
   "source": [
    "# Error and Label Lists\n",
    "errors = [0 if y_pred[ix,0] == y_test.values[ix] else 1 for ix in range(len(y_pred[:,0]))]\n",
    "labels = [y_test.values[ix] for ix in range(len(y_pred[:,0]))]\n",
    "\n",
    "# Visualization using UMAP\n",
    "umap.plot.points(mapper, labels=np.array(errors), theme='fire')\n",
    "plt.gca().set_title('UMAP Visualization Highlighting Prediction Errors')\n",
    "plt.xlabel('UMAP 1st Component')\n",
    "plt.ylabel('UMAP 2nd Component')\n",
    "plt.show()\n",
    "\n",
    "umap.plot.points(mapper, labels=np.array(labels), theme='fire')\n",
    "plt.gca().set_title('UMAP Visualization with True Labels')\n",
    "plt.xlabel('UMAP 1st Component')\n",
    "plt.ylabel('UMAP 2nd Component')\n",
    "plt.show()\n",
    "\n",
    "# # Scatter Plot for 2D UMAP Visualization\n",
    "# x_coords = embedding[:, 0]\n",
    "# y_coords = embedding[:, 1]\n",
    "# plt.scatter(x_coords, y_coords, c=labels, cmap='jet', s=15, alpha=0.6, edgecolors='w')\n",
    "# plt.xlabel('UMAP 1st component')\n",
    "# plt.ylabel('UMAP 2nd component')\n",
    "# plt.colorbar(label='Label')\n",
    "# plt.title('2D UMAP Visualization with Labels')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QQnLNDKaWomw",
   "metadata": {
    "id": "QQnLNDKaWomw"
   },
   "source": [
    "#### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Es54yR81O14Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Es54yR81O14Z",
    "outputId": "8c7b42dd-70f3-4145-ef86-e004ecd16db4"
   },
   "outputs": [],
   "source": [
    "# SHAP analysis\n",
    "import shap\n",
    "\n",
    "# Convert pandas DataFrame to numpy array if needed\n",
    "X_train_scaled_new_array = np.array(X_train_scaled_new)\n",
    "X_test_scaled_new_array = np.array(X_test_scaled_new)\n",
    "\n",
    "# Use a smaller random sample for background\n",
    "background_sample = X_train_scaled_new_array[np.random.choice(X_train_scaled_new_array.shape[0], 1000, replace=False)]\n",
    "\n",
    "# Initialize the explainer object\n",
    "explainer = shap.DeepExplainer(model, background_sample)\n",
    "\n",
    "# Compute the SHAP values\n",
    "shap_values = explainer.shap_values(X_test_scaled_new_array)\n",
    "\n",
    "# Summary plot to show feature importance for the entire dataset\n",
    "shap.summary_plot(shap_values, X_test_scaled_new_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hsajMNUHO160",
   "metadata": {
    "id": "hsajMNUHO160"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frpV3R9ZNwVf",
   "metadata": {
    "id": "frpV3R9ZNwVf"
   },
   "source": [
    "## Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iDRdGR_4NsSB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDRdGR_4NsSB",
    "outputId": "a697d0a1-5f6e-41cb-8ac6-3912613dee13"
   },
   "outputs": [],
   "source": [
    "# Adjust class weight\n",
    "# class_weight_0 = len(y_train) / (2 * np.sum(y_train == 0))\n",
    "# class_weight_1 = len(y_train) / (2 * np.sum(y_train == 1))\n",
    "# class_weights = {0: class_weight_0, 1: class_weight_1}\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Performance Scheduling\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train_scaled.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=64, sample_weight=sample_weights, validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts8L5mrOIxB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tts8L5mrOIxB",
    "outputId": "e6dce326-db97-41d1-8f23-fb6217b8e0db"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wm434JB0P52-",
   "metadata": {
    "id": "Wm434JB0P52-"
   },
   "source": [
    "Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yZADAYKpP3WQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "yZADAYKpP3WQ",
    "outputId": "60e2e006-6f6f-43de-b682-b0eaee4ee0eb"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    # Extract the training and validation losses & accuracies\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training and validation accuracies\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_acc, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mu3jIyojQGiH",
   "metadata": {
    "id": "mu3jIyojQGiH"
   },
   "source": [
    "**DNN Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zSaNWJ-UZDEM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSaNWJ-UZDEM",
    "outputId": "0b843768-2fe1-4659-e484-441de65305f1"
   },
   "outputs": [],
   "source": [
    "# train and test predictions\n",
    "dnn_y_test_pred = model.predict(X_test_scaled)\n",
    "dnn_y_train_pred = model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wuewJwfXboj0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuewJwfXboj0",
    "outputId": "769948e7-32e0-489a-aa91-64e4f6cb85b6"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, dnn_y_test_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KbSOv5nmVTSx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbSOv5nmVTSx",
    "outputId": "60718602-0851-4cac-dbd1-d2306df662de"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "print(f\"{balanced_accuracy_score(y_test, dnn_y_test_pred.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, dnn_y_train_pred.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TK6e8f89ZsZc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "TK6e8f89ZsZc",
    "outputId": "705ec0b7-2add-451d-b9df-2d89f892e233"
   },
   "outputs": [],
   "source": [
    "# define labels\n",
    "labels = ['0', '1']\n",
    "\n",
    "# confusion matrix\n",
    "test_cm = confusion_matrix(y_test, dnn_y_test_pred.round())\n",
    "#train_cm = confusion_matrix(y_train, dnn_y_train_pred.round())\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_title('Confusion Matrix for Test Set')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OWN68c9Nq-l9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "OWN68c9Nq-l9",
    "outputId": "01f3fcbd-0397-43ba-ed63-64ce04e2b1b9"
   },
   "outputs": [],
   "source": [
    "# calibration curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, dnn_y_test_pred, n_bins=10, strategy='uniform')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')\n",
    "plt.plot(prob_pred, prob_true, 's-', label='Calibration curve')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.xlabel('Mean Predicted Value')\n",
    "plt.legend()\n",
    "plt.title('Calibration Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m01v0SzCsl8V",
   "metadata": {
    "id": "m01v0SzCsl8V"
   },
   "source": [
    "Perfectly Calibrated: In a perfectly calibrated model, the calibration curve would be a diagonal line, going from the bottom left corner to the top right corner of the plot (i.e., a 45-degree line, often labeled as \"Perfectly calibrated\"). This would mean that for items where the model predicts, say, a 70% chance, 70% of those items actually belong to the positive class.\n",
    "\n",
    "Over-confident: If the curve is below the diagonal line, the model's predicted probabilities are generally higher than the actual fraction of positives, indicating over-confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uJ0hwgBGasVA",
   "metadata": {
    "id": "uJ0hwgBGasVA"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bt4C8Q8iVZXM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "bt4C8Q8iVZXM",
    "outputId": "76cebf1c-adb5-4935-ff97-62c41c742520"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3ozgTIta1WX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3ozgTIta1WX",
    "outputId": "2b76be64-d2ad-4bb3-e624-f99ef9dfb495"
   },
   "outputs": [],
   "source": [
    "rf_y_pred = rf.predict(X_test_scaled)\n",
    "rf_y_train_pred = rf.predict(X_train_scaled)\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hnjBtxXGcnnj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnjBtxXGcnnj",
    "outputId": "8c38a243-1bfc-4c50-8df7-aeab67843dd3"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(f\"{balanced_accuracy_score(y_test, rf_y_pred.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, rf_y_train_pred.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6XoguqCzqJ0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "6XoguqCzqJ0f",
    "outputId": "8531fd8a-b098-4131-950c-d9f7b1730642"
   },
   "outputs": [],
   "source": [
    "# define labels\n",
    "labels = ['0', '1']\n",
    "\n",
    "# confusion matrix\n",
    "test_cm = confusion_matrix(y_test, rf_y_pred.round())\n",
    "train_cm = confusion_matrix(y_train, rf_y_train_pred.round())\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_title('Confusion Matrix for Test Set')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PcM3jy5mGF09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "PcM3jy5mGF09",
    "outputId": "ba5da0d0-273d-4338-b8a1-5d2b56e584b8"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualizing the first decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(rf.estimators_[0], filled=True, max_depth=2, feature_names=X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zwRk8Co6GMq-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zwRk8Co6GMq-",
    "outputId": "5eb700dd-e02a-4d4f-958a-ecae372d8003"
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Get the indices of the top 50 features\n",
    "sorted_indices = np.argsort(importances)[-50:]\n",
    "\n",
    "plt.figure(figsize=(10,18))\n",
    "plt.barh(features[sorted_indices], importances[sorted_indices])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 50 Feature Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RxP-mSEXbYUR",
   "metadata": {
    "id": "RxP-mSEXbYUR"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zUxFK2TYa8iB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUxFK2TYa8iB",
    "outputId": "3aed5370-f93a-4cf6-8096-32aaed95e416"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict using test set\n",
    "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "xgb_train_predictions = xgb_model.predict(X_train_scaled)\n",
    "\n",
    "# show classification report\n",
    "print(\"XGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H67t9_n3bjcq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H67t9_n3bjcq",
    "outputId": "be34a99e-7dc3-4d43-a63b-95d78b27afb4"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(f\"{balanced_accuracy_score(y_test, xgb_predictions.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, xgb_train_predictions.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T7DnvBe-qUoD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "T7DnvBe-qUoD",
    "outputId": "07cf0eb6-b571-47b2-b46f-ff41b15b9b19"
   },
   "outputs": [],
   "source": [
    "# define labels\n",
    "labels = ['0', '1']\n",
    "\n",
    "# confusion matrix\n",
    "test_cm = confusion_matrix(y_test, xgb_predictions.round())\n",
    "train_cm = confusion_matrix(y_train, xgb_train_predictions.round())\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_title('XGBoost Confusion Matrix for Test Set')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0EMRPnJpqP3",
   "metadata": {
    "id": "f0EMRPnJpqP3"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYlW1NTud1yl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYlW1NTud1yl",
    "outputId": "04434f06-3c04-4779-a245-5b834d9e307f"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Create and train the model\n",
    "lgb_model = lgb.LGBMClassifier(objective=\"binary\", random_state=42)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict using test set\n",
    "lgb_predictions = lgb_model.predict(X_test_scaled)\n",
    "lgb_train_predictions = lgb_model.predict(X_train_scaled)\n",
    "\n",
    "# show classification report\n",
    "print(\"LightGBM Classification Report:\\n\")\n",
    "print(classification_report(y_test, lgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HV5Rg3uup7ih",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV5Rg3uup7ih",
    "outputId": "141df3e0-de07-49ed-a7df-f8ed4060f3c5"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(f\"{balanced_accuracy_score(y_test, lgb_predictions.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, lgb_train_predictions.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rKx_XPAp_yA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "8rKx_XPAp_yA",
    "outputId": "fc2f5660-e90e-4da3-8d1b-f94a49889c25"
   },
   "outputs": [],
   "source": [
    "# define labels\n",
    "labels = ['0', '1']\n",
    "\n",
    "# confusion matrix\n",
    "test_cm = confusion_matrix(y_test, lgb_predictions.round())\n",
    "train_cm = confusion_matrix(y_train, lgb_train_predictions.round())\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_title('Light GBM Confusion Matrix for Test Set')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ps8rM4U_s-Dp",
   "metadata": {
    "id": "ps8rM4U_s-Dp"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2rGXbNdps_gt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rGXbNdps_gt",
    "outputId": "9737b90e-c36a-4cfa-94d1-bc4b4e6dbf78"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create and train the model with balanced class weights and increased iterations\n",
    "logistic_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using test set\n",
    "logistic_predictions = logistic_model.predict(X_test_scaled)\n",
    "logistic_train_predictions = logistic_model.predict(X_train_scaled)\n",
    "\n",
    "# Show classification report\n",
    "print(\"Logistic Regression Classification Report:\\n\")\n",
    "print(classification_report(y_test, logistic_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YcST2QV2tGqg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcST2QV2tGqg",
    "outputId": "2e426905-c39a-4013-e7e5-2093c75cf2d7"
   },
   "outputs": [],
   "source": [
    "# calculate balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(f\"{balanced_accuracy_score(y_test, logistic_predictions.round()) * 100:.2f}% - Balanced accuracy Test\")\n",
    "print(f\"{balanced_accuracy_score(y_train, logistic_train_predictions.round()) * 100:.2f}% - Balanced accuracy Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boQRya2etRz_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "boQRya2etRz_",
    "outputId": "af8dd758-0647-4576-ea18-b829df4c9141"
   },
   "outputs": [],
   "source": [
    "# define labels\n",
    "labels = ['0', '1']\n",
    "\n",
    "# confusion matrix\n",
    "test_cm = confusion_matrix(y_test, logistic_predictions.round())\n",
    "train_cm = confusion_matrix(y_train, logistic_train_predictions.round())\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set_title('Logistic Regression Confusion Matrix for Test Set')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i3b1xbRb5p_u",
   "metadata": {
    "id": "i3b1xbRb5p_u"
   },
   "source": [
    "**SHAP (SHapley Additive exPlanations)**\n",
    "\n",
    "Explain the output of machine learning models and identifying feature importance by using Shapley values to fairly allocate contributions of each feature for individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QeMvXLpO6QWP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeMvXLpO6QWP",
    "outputId": "272af8c9-9062-46d2-db25-e98666bbd574"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O9wjZ0jo9Qis",
   "metadata": {
    "id": "O9wjZ0jo9Qis"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# # Convert training and test sets to DataFrames with those feature names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "# Initialize the explainer object with the appropriate masker\n",
    "masker = shap.maskers.Independent(data=X_train_scaled)\n",
    "explainer = shap.LinearExplainer(logistic_model, masker=masker)\n",
    "\n",
    "# Compute SHAP values for a particular set (e.g., test set)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# force plot for the first instance\n",
    "# shap.initjs()\n",
    "# shap.plots.force(shap_values[0])\n",
    "\n",
    "# Summary plot to show feature importance for the entire dataset\n",
    "shap.summary_plot(shap_values, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vUnSnVYtXUw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "id": "0vUnSnVYtXUw",
    "outputId": "340fe9c2-cae0-4139-c4e0-4cc8cf79e45b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "xSFjCVEJAv1B",
   "metadata": {
    "id": "xSFjCVEJAv1B"
   },
   "source": [
    "Red: Feature has a high value for that observation\n",
    "\n",
    "Blue: Feature has a low value for that observation\n",
    "\n",
    "Position on the x-axis: Indicates the effect of that value on the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ziHvCiD8900u",
   "metadata": {
    "id": "ziHvCiD8900u"
   },
   "source": [
    "## ROC Curve for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G7qda2-X925_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "G7qda2-X925_",
    "outputId": "3d44e945-7184-4d90-fc16-a9fc705acd01"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# For DNN\n",
    "y_pred_dnn = model.predict(X_test_scaled).ravel()\n",
    "fpr_dnn, tpr_dnn, thresholds_dnn = roc_curve(y_test, y_pred_dnn)\n",
    "roc_auc_dnn = auc(fpr_dnn, tpr_dnn)\n",
    "\n",
    "# For Random Forest\n",
    "y_pred_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# For XGBoost\n",
    "y_pred_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "\n",
    "# For LightGBM\n",
    "y_pred_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_lgb, tpr_lgb, thresholds_lgb = roc_curve(y_test, y_pred_lgb)\n",
    "roc_auc_lgb = auc(fpr_lgb, tpr_lgb)\n",
    "\n",
    "# For Logistic Regression\n",
    "y_pred_logistic = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_logistic, tpr_logistic, thresholds_logistic = roc_curve(y_test, y_pred_logistic)\n",
    "roc_auc_logistic = auc(fpr_logistic, tpr_logistic)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr_dnn, tpr_dnn, label=f'DNN (area = {roc_auc_dnn:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (area = {roc_auc_xgb:.2f})')\n",
    "plt.plot(fpr_lgb, tpr_lgb, label=f'LightGBM (area = {roc_auc_lgb:.2f})')\n",
    "plt.plot(fpr_logistic, tpr_logistic, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
